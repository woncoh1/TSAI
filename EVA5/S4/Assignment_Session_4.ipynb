{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Session_4_EVA5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woncoh1/EVA5/blob/main/Assignment_Session_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmiRv0WJPC9M"
      },
      "source": [
        "# Instructions\n",
        "1. We have considered many many points in our last 4 lectures. Some of these we have covered directly and some indirectly. They are:\n",
        "  1. How many layers,\n",
        "  2. MaxPooling,\n",
        "  3. 1x1 Convolutions,\n",
        "  4. 3x3 Convolutions,\n",
        "  5. Receptive Field,\n",
        "  6. SoftMax,\n",
        "  7. Learning Rate,\n",
        "  8. Kernels and how do we decide the number of kernels?\n",
        "  9. Batch Normalization,\n",
        "  10. Image Normalization,\n",
        "  11. Position of MaxPooling,\n",
        "  12. Concept of Transition Layers,\n",
        "  13. Position of Transition Layer,\n",
        "  14. DropOut\n",
        "  15. When do we introduce DropOut, or when do we know we have some overfitting\n",
        "  16. The distance of MaxPooling from Prediction,\n",
        "  17. The distance of Batch Normalization from Prediction,\n",
        "  18. When do we stop convolutions and go ahead with a larger kernel or some other alternative (GAP, which we have not yet covered)\n",
        "  19. How do we know our network is not going well, comparatively, very early\n",
        "  20. Batch Size, and effects of batch size\n",
        "  21. Etc (you can add more if we missed it here)\n",
        "2. Refer to this code: [COLABLINK](https://colab.research.google.com/drive/1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx). WRITE IT AGAIN SUCH THAT IT ACHIEVES:\n",
        "    1. You can use anything from above you want. \n",
        "    2. **99.4% validation (test) accuracy**\n",
        "    3. **Less than 20k parameters**\n",
        "    4. **Less than 20 epochs**\n",
        "    5. **No fully connected layer**\n",
        "    6. To learn how to add different things we covered in this session, you can refer to this code: https://www.kaggle.com/enwei26/mnist-digits-pytorch-cnn-99 DONT COPY ARCHITECTURE, JUST LEARN HOW TO INTEGRATE THINGS LIKE DROPOUT, BATCHNORM, ETC.\n",
        "---\n",
        "**Submission details**\n",
        "3. This is a slightly time-consuming assignment, please make sure you start early. You are going to spend a lot of effort into running the programs multiple times\n",
        "4. Once you are done, submit your results in S4-Assignment-Solution\n",
        "5. You must upload your assignment to a public GitHub Repository. Create a folder called S4 in it, and add your iPynb code in it. THE LOGS MUST BE VISIBLE. Before adding the link to the submission make sure you have opened the file in an \"incognito\" window. \n",
        "6. If you misrepresent your answers, you will be awarded -100% of the score.\n",
        "7. If you submit Colab Link instead of notebook uploaded on GitHub, or redirect the GitHub page to colab, you will be awarded -50%\n",
        "8. This assignment is worth 300pts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvSUN8Ak7YQZ"
      },
      "source": [
        "# define hyperparameters (constants)\n",
        "\n",
        "# data\n",
        "BATCH_SIZE = 128\n",
        "# model\n",
        "DROPOUT_RATE = 0.1\n",
        "# optimizer\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "# training\n",
        "EPOCHS = 20"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTsEoRwJZcqG"
      },
      "source": [
        "# 1. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "# Construct batches of training and testing data\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(1) \n",
        "\n",
        "# GPU configurations for data loaders\n",
        "use_cuda = torch.cuda.is_available()\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# divide and process training dataset into batch iterator\n",
        "train_dataset = datasets.MNIST(\n",
        "    '../data', \n",
        "    train=True,  \n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                  ])\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "# divide and process testing dataset into batch iterator\n",
        "test_dataset = datasets.MNIST(\n",
        "    '../data', \n",
        "     train=False,  \n",
        "     download=False,\n",
        "     transform=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                   ])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True, \n",
        "    **kwargs\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5BOKd6OZnAe"
      },
      "source": [
        "# 2. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "# Define model (neural network) structure\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    # prepare layers and blocks\n",
        "    def __init__(self, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3x3 feature extraction block #1\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=8),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        # 3x3 feature extraction block #2\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        # 2x2 max pooling #1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 1x1 convolution for reducing # of parameters\n",
        "        self.trans = nn.Conv2d(in_channels=16, out_channels=8, \n",
        "                                kernel_size=1, padding=1)\n",
        "        \n",
        "        # 3x3 feature extraction block #3\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # 3x3 feature extraction block #4\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # 2x2 max pooling #2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 3x3 feature extraction block #5\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "        \n",
        "        # 3x3 feature extraction block #6\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, \n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.Dropout2d(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # 1x1 convolution for reducing # of output channels to # of classes\n",
        "        # CEO, so no ReLU, batch norm, dropout, etc\n",
        "        self.conv7 = nn.Conv2d(in_channels=32, out_channels=10,\n",
        "                               kernel_size=1, padding=1)\n",
        "        \n",
        "        # global average pooling instead of fully connected layer\n",
        "        self.gap = nn.AvgPool2d(kernel_size=10)\n",
        "    \n",
        "    # assemble layers and blocks\n",
        "    def forward(self, x):\n",
        "        # input layer\n",
        "        x = x\n",
        "        # hidden layers\n",
        "        x = self.conv1(x) # Input: 28 x 28 x 01 | Kernel: 03 x 03 x 01 x 08 | Output: 28 x 28 x 08, RF: 3\n",
        "        x = self.conv2(x) # Input: 28 x 28 x 08 | Kernel: 03 x 03 x 08 x 16 | Output: 28 x 28 x 16, RF: 5\n",
        "        x = self.pool1(x) # Input: 28 x 28 x 16 | Kernel: 02 x 02 x 01 x 16 | Output: 14 x 14 x 16, RF: 10 \n",
        "        x = self.trans(x) # Input: 14 x 14 x 16 | Kernel: 01 x 01 x 16 x 08 | Output: 16 x 16 x 08, RF: 10\n",
        "        x = self.conv3(x) # Input: 16 x 16 x 08 | Kernel: 03 x 03 x 08 x 16 | Output: 16 x 16 x 16, RF: 12\n",
        "        x = self.conv4(x) # Input: 16 x 16 x 16 | Kernel: 03 x 03 x 16 x 16 | Output: 16 x 16 x 16, RF: 14\n",
        "        x = self.pool2(x) # Input: 16 x 16 x 16 | Kernel: 02 x 02 x 01 x 16 | Output: 08 x 08 x 16, RF: 28\n",
        "        x = self.conv5(x) # Input: 08 x 08 x 16 | Kernel: 03 x 03 x 16 x 32 | Output: 08 x 08 x 32, RF: 30\n",
        "        x = self.conv6(x) # Input: 08 x 08 x 32 | Kernel: 03 x 03 x 32 x 32 | Output: 08 x 08 x 32, RF: 32\n",
        "        x = self.conv7(x) # Input: 08 x 08 x 32 | Kernel: 01 x 01 x 32 x 10 | Output: 10 x 10 x 10, RF: 34\n",
        "        x = self.gap(x)   # Input: 10 x 10 x 10 | Kernel: 10 x 10 x 01 x 10 | Output: 01 x 01 x 10, RF: 34\n",
        "        # output layer\n",
        "        x = x.view(-1, 10) # flatten (# of classes = 10)\n",
        "        return F.log_softmax(x, dim=1) # fed to NLL loss for cross entropy loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fd99c2-a0c0-4cf1-fd36-2e6165139064"
      },
      "source": [
        "# Instantiate model into device and\n",
        "# view model summary\n",
        "\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net(DROPOUT_RATE).to(device)\n",
        "summary(model, input_size=(1, 28, 28)) # default input size of MNIST dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "              ReLU-2            [-1, 8, 28, 28]               0\n",
            "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
            "         Dropout2d-4            [-1, 8, 28, 28]               0\n",
            "            Conv2d-5           [-1, 16, 28, 28]           1,168\n",
            "              ReLU-6           [-1, 16, 28, 28]               0\n",
            "       BatchNorm2d-7           [-1, 16, 28, 28]              32\n",
            "         Dropout2d-8           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-9           [-1, 16, 14, 14]               0\n",
            "           Conv2d-10            [-1, 8, 16, 16]             136\n",
            "           Conv2d-11           [-1, 16, 16, 16]           1,168\n",
            "             ReLU-12           [-1, 16, 16, 16]               0\n",
            "      BatchNorm2d-13           [-1, 16, 16, 16]              32\n",
            "        Dropout2d-14           [-1, 16, 16, 16]               0\n",
            "           Conv2d-15           [-1, 16, 16, 16]           2,320\n",
            "             ReLU-16           [-1, 16, 16, 16]               0\n",
            "      BatchNorm2d-17           [-1, 16, 16, 16]              32\n",
            "        Dropout2d-18           [-1, 16, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 16, 8, 8]               0\n",
            "           Conv2d-20             [-1, 32, 8, 8]           4,640\n",
            "             ReLU-21             [-1, 32, 8, 8]               0\n",
            "      BatchNorm2d-22             [-1, 32, 8, 8]              64\n",
            "        Dropout2d-23             [-1, 32, 8, 8]               0\n",
            "           Conv2d-24             [-1, 32, 8, 8]           9,248\n",
            "             ReLU-25             [-1, 32, 8, 8]               0\n",
            "      BatchNorm2d-26             [-1, 32, 8, 8]              64\n",
            "        Dropout2d-27             [-1, 32, 8, 8]               0\n",
            "           Conv2d-28           [-1, 10, 10, 10]             330\n",
            "        AvgPool2d-29             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 19,330\n",
            "Trainable params: 19,330\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.00\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.08\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7rz4mOEfhjr"
      },
      "source": [
        "# 3. Train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_ukfAY1p_vq"
      },
      "source": [
        "# Instantiate criterion\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Instantiate optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "# Define how to train and test model\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Optimize model parameters\n",
        "def train(model: 'neural network', \n",
        "          device: 'CPU or GPU', \n",
        "          train_loader: 'training batch', \n",
        "          optimizer: 'weight-optimizing algorithm', \n",
        "          epoch: 'how many cycles of consuming the entire batches'):\n",
        "  \n",
        "    # set model to train mode, enabling features like regularization\n",
        "    model.train() \n",
        "    # instantiate progress meter\n",
        "    pbar = tqdm(train_loader)\n",
        "    # iterate for every batch\n",
        "    for batch_idx, (data, target) in enumerate(pbar): \n",
        "        # instantiate data and target to device\n",
        "        data, target = data.to(device), target.to(device) \n",
        "        \n",
        "        # zero previously accumulated gradients for mini-batch update\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        output = model(data)\n",
        "        # calculate loss\n",
        "        loss = criterion(output, target) \n",
        "        # calculate gradients \n",
        "        loss.backward() \n",
        "        # update parameters\n",
        "        optimizer.step() \n",
        "        \n",
        "        # display progress meter                                            \n",
        "        pbar.set_description(desc=(f'epoch: {epoch + 1} | '\n",
        "                                   f'loss= {loss.item():.4f} | '\n",
        "                                   f'batch_id= {batch_idx}'))\n",
        "\n",
        "# Test how well model parameters are optimized\n",
        "def test(model, device, test_loader: 'testing batch'):\n",
        "\n",
        "    # set model to evaluation mode, disabling features like regularization\n",
        "    model.eval()\n",
        "    # iteratively accumulate test loss and # of correct predictions\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # no backprop\n",
        "    with torch.no_grad():\n",
        "        # iterate for every batch\n",
        "        for data, target in test_loader:\n",
        "            # instantiate data and target to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += criterion(output, target).sum().item()  \n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            # update # of correct predictions  \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    \n",
        "    # average test loss per batch\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # display results\n",
        "    print((f'\\nTest set: Average loss= {test_loss:.4f} | '\n",
        "           f'Accuracy= {correct}/{len(test_loader.dataset)} '\n",
        "           f'({100. * correct / len(test_loader.dataset):.1f}%)\\n'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28e8167-d05b-4d9e-f26e-0e61f13e5b66"
      },
      "source": [
        "# Bring batches, model, loss function and optimizer together to \n",
        "# carry out training and testing of the model\n",
        "for epoch in range(EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 | loss= 0.2395 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.41it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0014 | Accuracy= 9601/10000 (96.0%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 2 | loss= 0.2131 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.88it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0007 | Accuracy= 9755/10000 (97.5%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 3 | loss= 0.2334 | batch_id= 468: 100%|██████████| 469/469 [00:15<00:00, 29.65it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0004 | Accuracy= 9845/10000 (98.5%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 4 | loss= 0.0511 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.92it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0003 | Accuracy= 9875/10000 (98.8%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 5 | loss= 0.0807 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.55it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0003 | Accuracy= 9893/10000 (98.9%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 6 | loss= 0.1188 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.80it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0003 | Accuracy= 9907/10000 (99.1%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 7 | loss= 0.0321 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.14it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9915/10000 (99.2%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 8 | loss= 0.1501 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.17it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9910/10000 (99.1%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 9 | loss= 0.0788 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.11it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9913/10000 (99.1%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 10 | loss= 0.0495 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.23it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9915/10000 (99.2%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 11 | loss= 0.0868 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.62it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9921/10000 (99.2%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 12 | loss= 0.0492 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.09it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9927/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 13 | loss= 0.0655 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.18it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9928/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 14 | loss= 0.0714 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.99it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9929/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 15 | loss= 0.0498 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.10it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9930/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 16 | loss= 0.0351 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.30it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0001 | Accuracy= 9935/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 17 | loss= 0.0344 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 29.00it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0001 | Accuracy= 9930/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 18 | loss= 0.0680 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.99it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9929/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 19 | loss= 0.0349 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.47it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0001 | Accuracy= 9936/10000 (99.4%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 20 | loss= 0.0541 | batch_id= 468: 100%|██████████| 469/469 [00:16<00:00, 28.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss= 0.0002 | Accuracy= 9929/10000 (99.3%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
